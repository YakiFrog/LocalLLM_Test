#!/usr/bin/env python3
"""
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½
LLMã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„è¨­å®šã‚’ãƒ†ã‚¹ãƒˆãŠã‚ˆã³èª¿æ•´ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«
"""

import json
import os
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime

from main import LMStudioClient

class PromptTuner:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_url="http://127.0.0.1:1234"):
        """
        åˆæœŸåŒ–
        
        Args:
            base_url: LM Studioã®ãƒ™ãƒ¼ã‚¹URL
        """
        self.client = LMStudioClient(base_url)
        self.config_file = Path("prompt_configs.json")
        self.test_results_file = Path("prompt_test_results.json")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        self.configs = self.load_configs()
        
        # ãƒ†ã‚¹ãƒˆçµæœå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰
        self.test_results = self.load_test_results()
    
    def load_configs(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½œæˆ
            default_configs = {
                "system_messages": {
                    "default": "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥çš„ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€Œã‚·ãƒªã‚¦ã‚¹ã€ã§ã™ã€‚è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚",
                    "formal": "ã‚ãªãŸã¯å°‚é–€çš„ã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€Œã‚·ãƒªã‚¦ã‚¹ã€ã§ã™ã€‚ä¸å¯§ã§è©³ç´°ãªæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚",
                    "casual": "ã‚ãªãŸã¯è¦ªã—ã¿ã‚„ã™ããƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€Œã‚·ãƒªã‚¦ã‚¹ã€ã§ã™ã€‚ãã ã‘ãŸæ—¥æœ¬èªã§æ¥½ã—ãä¼šè©±ã—ã¦ãã ã•ã„ã€‚",
                    "technical": "ã‚ãªãŸã¯æŠ€è¡“çš„ãªå°‚é–€çŸ¥è­˜ã‚’æŒã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€Œã‚·ãƒªã‚¦ã‚¹ã€ã§ã™ã€‚æ­£ç¢ºã§è©³ç´°ãªæŠ€è¡“æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
                    "creative": "ã‚ãªãŸã¯å‰µé€ çš„ã§æƒ³åƒåŠ›è±Šã‹ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€Œã‚·ãƒªã‚¦ã‚¹ã€ã§ã™ã€‚ã‚¢ã‚¤ãƒ‡ã‚¢ã‚„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’è€ƒãˆã‚‹ã®ãŒå¾—æ„ã§ã™ã€‚"
                },
                "llm_settings": {
                    "default": {
                        "model": "openai/gpt-oss-20b",
                        "temperature": 0.7,
                        "max_tokens": -1
                    },
                    "conservative": {
                        "model": "openai/gpt-oss-20b",
                        "temperature": 0.3,
                        "max_tokens": 500
                    },
                    "creative": {
                        "model": "openai/gpt-oss-20b",
                        "temperature": 0.9,
                        "max_tokens": 1000
                    },
                    "precise": {
                        "model": "openai/gpt-oss-20b",
                        "temperature": 0.1,
                        "max_tokens": 300
                    }
                },
                "test_scenarios": [
                    {
                        "name": "åŸºæœ¬æŒ¨æ‹¶",
                        "user_message": "ã“ã‚“ã«ã¡ã¯ï¼",
                        "expected_style": "è¦ªã—ã¿ã‚„ã™ã„æŒ¨æ‹¶"
                    },
                    {
                        "name": "æŠ€è¡“è³ªå•",
                        "user_message": "Pythonã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’æ•™ãˆã¦ãã ã•ã„",
                        "expected_style": "æŠ€è¡“çš„ã§æ­£ç¢ºãªèª¬æ˜"
                    },
                    {
                        "name": "å‰µä½œä¾é ¼",
                        "user_message": "çŸ­ã„ç‰©èªã‚’ä½œã£ã¦ãã ã•ã„",
                        "expected_style": "å‰µé€ çš„ã§é­…åŠ›çš„ãªå†…å®¹"
                    },
                    {
                        "name": "æ„Ÿæƒ…çš„ãªç›¸è«‡",
                        "user_message": "æœ€è¿‘ç–²ã‚Œã¦ã„ã¦å…ƒæ°—ãŒå‡ºã¾ã›ã‚“",
                        "expected_style": "å…±æ„Ÿçš„ã§åŠ±ã¾ã—ã®ã‚ã‚‹è¿”ç­”"
                    }
                ]
            }
            self.save_configs(default_configs)
            return default_configs
    
    def save_configs(self, configs: Dict[str, Any]):
        """è¨­å®šã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(configs, f, ensure_ascii=False, indent=2)
        self.configs = configs
    
    def load_test_results(self) -> List[Dict[str, Any]]:
        """ãƒ†ã‚¹ãƒˆçµæœå±¥æ­´ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if self.test_results_file.exists():
            with open(self.test_results_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def save_test_results(self):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        with open(self.test_results_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2)
    
    def add_system_message(self, name: str, message: str):
        """æ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ """
        self.configs["system_messages"][name] = message
        self.save_configs(self.configs)
        print(f"âœ… ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ '{name}' ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    
    def add_llm_setting(self, name: str, model: str, temperature: float, max_tokens: int):
        """æ–°ã—ã„LLMè¨­å®šã‚’è¿½åŠ """
        self.configs["llm_settings"][name] = {
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        self.save_configs(self.configs)
        print(f"âœ… LLMè¨­å®š '{name}' ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    
    def test_prompt_combination(self, 
                               system_message_name: str, 
                               llm_setting_name: str, 
                               user_message: str,
                               scenario_name: str = "manual_test") -> Dict[str, Any]:
        """
        ç‰¹å®šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿åˆã‚ã›ã‚’ãƒ†ã‚¹ãƒˆ
        
        Args:
            system_message_name: ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å
            llm_setting_name: LLMè¨­å®šå
            user_message: ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            scenario_name: ã‚·ãƒŠãƒªã‚ªå
            
        Returns:
            ãƒ†ã‚¹ãƒˆçµæœ
        """
        if system_message_name not in self.configs["system_messages"]:
            raise ValueError(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ '{system_message_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        if llm_setting_name not in self.configs["llm_settings"]:
            raise ValueError(f"LLMè¨­å®š '{llm_setting_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # è¨­å®šã‚’å–å¾—
        system_message = self.configs["system_messages"][system_message_name]
        llm_setting = self.configs["llm_settings"][llm_setting_name]
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
        
        # LLMã«é€ä¿¡
        try:
            start_time = datetime.now()
            response = self.client.chat_completion(
                messages=messages,
                model=llm_setting["model"],
                temperature=llm_setting["temperature"],
                max_tokens=llm_setting["max_tokens"]
            )
            end_time = datetime.now()
            
            if response and "choices" in response:
                ai_response = response["choices"][0]["message"]["content"]
                
                # ãƒ†ã‚¹ãƒˆçµæœã‚’è¨˜éŒ²
                test_result = {
                    "timestamp": start_time.isoformat(),
                    "scenario_name": scenario_name,
                    "system_message_name": system_message_name,
                    "llm_setting_name": llm_setting_name,
                    "user_message": user_message,
                    "ai_response": ai_response,
                    "response_time_seconds": (end_time - start_time).total_seconds(),
                    "success": True,
                    "token_count": response.get("usage", {}).get("total_tokens", 0) if "usage" in response else 0
                }
                
                self.test_results.append(test_result)
                self.save_test_results()
                
                return test_result
            else:
                error_result = {
                    "timestamp": start_time.isoformat(),
                    "scenario_name": scenario_name,
                    "system_message_name": system_message_name,
                    "llm_setting_name": llm_setting_name,
                    "user_message": user_message,
                    "ai_response": None,
                    "response_time_seconds": (end_time - start_time).total_seconds(),
                    "success": False,
                    "error": "ç„¡åŠ¹ãªå¿œç­”"
                }
                
                self.test_results.append(error_result)
                self.save_test_results()
                
                return error_result
                
        except Exception as e:
            error_result = {
                "timestamp": datetime.now().isoformat(),
                "scenario_name": scenario_name,
                "system_message_name": system_message_name,
                "llm_setting_name": llm_setting_name,
                "user_message": user_message,
                "ai_response": None,
                "response_time_seconds": 0,
                "success": False,
                "error": str(e)
            }
            
            self.test_results.append(error_result)
            self.save_test_results()
            
            return error_result
    
    def run_full_test_suite(self) -> List[Dict[str, Any]]:
        """å…¨ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã‚’å®Ÿè¡Œ"""
        print("ğŸš€ ãƒ•ãƒ«ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹...")
        
        results = []
        total_tests = len(self.configs["test_scenarios"]) * len(self.configs["system_messages"]) * len(self.configs["llm_settings"])
        current_test = 0
        
        for scenario in self.configs["test_scenarios"]:
            for sys_msg_name in self.configs["system_messages"]:
                for llm_setting_name in self.configs["llm_settings"]:
                    current_test += 1
                    print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆ {current_test}/{total_tests}: {scenario['name']} Ã— {sys_msg_name} Ã— {llm_setting_name}")
                    
                    result = self.test_prompt_combination(
                        system_message_name=sys_msg_name,
                        llm_setting_name=llm_setting_name,
                        user_message=scenario["user_message"],
                        scenario_name=scenario["name"]
                    )
                    
                    results.append(result)
                    
                    if result["success"]:
                        print(f"  âœ… æˆåŠŸ ({result['response_time_seconds']:.2f}ç§’)")
                        print(f"  ğŸ“ å¿œç­”: {result['ai_response'][:100]}...")
                    else:
                        print(f"  âŒ å¤±æ•—: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
                    
                    print()
        
        print(f"ğŸ‰ ãƒ•ãƒ«ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†ï¼ {len(results)}ä»¶ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã—ãŸ")
        return results
    
    def analyze_results(self, scenario_name: Optional[str] = None) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æ"""
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if scenario_name:
            filtered_results = [r for r in self.test_results if r["scenario_name"] == scenario_name]
        else:
            filtered_results = self.test_results
        
        if not filtered_results:
            return {"error": "åˆ†æå¯¾è±¡ã®ãƒ†ã‚¹ãƒˆçµæœãŒã‚ã‚Šã¾ã›ã‚“"}
        
        # æˆåŠŸç‡è¨ˆç®—
        successful_results = [r for r in filtered_results if r["success"]]
        success_rate = len(successful_results) / len(filtered_results) * 100
        
        # å¹³å‡å¿œç­”æ™‚é–“è¨ˆç®—
        avg_response_time = sum(r["response_time_seconds"] for r in successful_results) / len(successful_results) if successful_results else 0
        
        # è¨­å®šåˆ¥æˆåŠŸç‡
        setting_stats = {}
        for result in filtered_results:
            key = f"{result['system_message_name']} Ã— {result['llm_setting_name']}"
            if key not in setting_stats:
                setting_stats[key] = {"total": 0, "success": 0}
            setting_stats[key]["total"] += 1
            if result["success"]:
                setting_stats[key]["success"] += 1
        
        # æˆåŠŸç‡ã§ã‚½ãƒ¼ãƒˆ
        sorted_settings = sorted(
            setting_stats.items(),
            key=lambda x: x[1]["success"] / x[1]["total"],
            reverse=True
        )
        
        analysis = {
            "total_tests": len(filtered_results),
            "successful_tests": len(successful_results),
            "success_rate_percent": success_rate,
            "average_response_time_seconds": avg_response_time,
            "best_performing_settings": sorted_settings[:5],
            "worst_performing_settings": sorted_settings[-5:] if len(sorted_settings) > 5 else []
        }
        
        return analysis
    
    def interactive_tuning(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"""
        print("ğŸ›ï¸  ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
        print("åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰:")
        print("  1. test - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ†ã‚¹ãƒˆ")
        print("  2. add_system - ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ")
        print("  3. add_setting - LLMè¨­å®šã‚’è¿½åŠ ")
        print("  4. run_suite - ãƒ•ãƒ«ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ")
        print("  5. analyze - çµæœåˆ†æ")
        print("  6. list - è¨­å®šä¸€è¦§è¡¨ç¤º")
        print("  7. quit - çµ‚äº†")
        print()
        
        while True:
            command = input("ã‚³ãƒãƒ³ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip().lower()
            
            if command == "quit" or command == "q":
                break
            elif command == "test" or command == "1":
                self._interactive_test()
            elif command == "add_system" or command == "2":
                self._interactive_add_system_message()
            elif command == "add_setting" or command == "3":
                self._interactive_add_llm_setting()
            elif command == "run_suite" or command == "4":
                self.run_full_test_suite()
            elif command == "analyze" or command == "5":
                self._interactive_analyze()
            elif command == "list" or command == "6":
                self._list_configurations()
            else:
                print("âŒ ç„¡åŠ¹ãªã‚³ãƒãƒ³ãƒ‰ã§ã™")
    
    def _interactive_test(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ†ã‚¹ãƒˆ"""
        print("\n--- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚¹ãƒˆ ---")
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é¸æŠ
        print("åˆ©ç”¨å¯èƒ½ãªã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:")
        for name in self.configs["system_messages"]:
            print(f"  - {name}")
        sys_msg_name = input("ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åã‚’å…¥åŠ›: ").strip()
        
        if sys_msg_name not in self.configs["system_messages"]:
            print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ '{sys_msg_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # LLMè¨­å®šé¸æŠ
        print("åˆ©ç”¨å¯èƒ½ãªLLMè¨­å®š:")
        for name in self.configs["llm_settings"]:
            print(f"  - {name}")
        llm_setting_name = input("LLMè¨­å®šåã‚’å…¥åŠ›: ").strip()
        
        if llm_setting_name not in self.configs["llm_settings"]:
            print(f"âŒ LLMè¨­å®š '{llm_setting_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¥åŠ›
        user_message = input("ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›: ").strip()
        
        if not user_message:
            print("âŒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        print("ğŸ”„ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        result = self.test_prompt_combination(sys_msg_name, llm_setting_name, user_message)
        
        if result["success"]:
            print(f"âœ… ãƒ†ã‚¹ãƒˆæˆåŠŸ ({result['response_time_seconds']:.2f}ç§’)")
            print(f"ğŸ“ AIå¿œç­”:\n{result['ai_response']}")
        else:
            print(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
        print()
    
    def _interactive_add_system_message(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ """
        print("\n--- ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ  ---")
        
        name = input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åã‚’å…¥åŠ›: ").strip()
        if not name:
            print("âŒ åå‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        message = input("ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›: ").strip()
        if not message:
            print("âŒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        self.add_system_message(name, message)
        print()
    
    def _interactive_add_llm_setting(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–LLMè¨­å®šè¿½åŠ """
        print("\n--- LLMè¨­å®šè¿½åŠ  ---")
        
        name = input("è¨­å®šåã‚’å…¥åŠ›: ").strip()
        if not name:
            print("âŒ åå‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        model = input("ãƒ¢ãƒ‡ãƒ«åã‚’å…¥åŠ› (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: openai/gpt-oss-20b): ").strip()
        if not model:
            model = "openai/gpt-oss-20b"
        
        try:
            temperature = float(input("Temperature (0-1, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7): ").strip() or "0.7")
            max_tokens = int(input("æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•° (-1ã§ç„¡åˆ¶é™, ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: -1): ").strip() or "-1")
        except ValueError:
            print("âŒ æ•°å€¤ã®å…¥åŠ›ãŒä¸æ­£ã§ã™")
            return
        
        self.add_llm_setting(name, model, temperature, max_tokens)
        print()
    
    def _interactive_analyze(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–çµæœåˆ†æ"""
        print("\n--- çµæœåˆ†æ ---")
        
        scenario = input("åˆ†æå¯¾è±¡ã‚·ãƒŠãƒªã‚ªå (ç©ºç™½ã§å…¨ä½“åˆ†æ): ").strip()
        if not scenario:
            scenario = None
        
        analysis = self.analyze_results(scenario)
        
        if "error" in analysis:
            print(f"âŒ {analysis['error']}")
            return
        
        print(f"ğŸ“Š åˆ†æçµæœ:")
        print(f"  ç·ãƒ†ã‚¹ãƒˆæ•°: {analysis['total_tests']}")
        print(f"  æˆåŠŸãƒ†ã‚¹ãƒˆæ•°: {analysis['successful_tests']}")
        print(f"  æˆåŠŸç‡: {analysis['success_rate_percent']:.1f}%")
        print(f"  å¹³å‡å¿œç­”æ™‚é–“: {analysis['average_response_time_seconds']:.2f}ç§’")
        
        print(f"\nğŸ† æœ€é«˜æ€§èƒ½è¨­å®š (ä¸Šä½5ä½):")
        for i, (setting, stats) in enumerate(analysis['best_performing_settings'], 1):
            success_rate = stats['success'] / stats['total'] * 100
            print(f"  {i}. {setting}: {success_rate:.1f}% ({stats['success']}/{stats['total']})")
        
        if analysis['worst_performing_settings']:
            print(f"\nâš ï¸  ä½æ€§èƒ½è¨­å®š (ä¸‹ä½5ä½):")
            for i, (setting, stats) in enumerate(analysis['worst_performing_settings'], 1):
                success_rate = stats['success'] / stats['total'] * 100
                print(f"  {i}. {setting}: {success_rate:.1f}% ({stats['success']}/{stats['total']})")
        
        print()
    
    def _list_configurations(self):
        """è¨­å®šä¸€è¦§è¡¨ç¤º"""
        print("\n--- è¨­å®šä¸€è¦§ ---")
        
        print("ğŸ“ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸:")
        for name, message in self.configs["system_messages"].items():
            print(f"  - {name}: {message[:50]}...")
        
        print("\nâš™ï¸  LLMè¨­å®š:")
        for name, setting in self.configs["llm_settings"].items():
            print(f"  - {name}: model={setting['model']}, temp={setting['temperature']}, tokens={setting['max_tokens']}")
        
        print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª ({len(self.configs['test_scenarios'])}ä»¶):")
        for scenario in self.configs["test_scenarios"]:
            print(f"  - {scenario['name']}: {scenario['user_message'][:30]}...")
        
        print()

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ›ï¸  ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
    print("=" * 50)
    
    tuner = PromptTuner()
    tuner.interactive_tuning()
    
    print("ğŸ‘‹ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã‚’çµ‚äº†ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main()