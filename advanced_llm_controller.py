#!/usr/bin/env python3
"""
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šçµ±åˆå‹LLMFaceController
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ã‚’çµ±åˆã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³
"""

import asyncio
import sys
import os
import json
import logging
from typing import Optional, Dict, Any, List
from pathlib import Path

# æ—¢å­˜ã®LLMFaceControllerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from llm_face_controller import LLMFaceController
from prompt_tuning import PromptTuner

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class AdvancedLLMFaceController(LLMFaceController):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½çµ±åˆå‹LLMFaceController"""
    
    def __init__(self, 
                 lm_studio_url="http://127.0.0.1:1234",
                 face_server_url="http://localhost:8080",
                 voicevox_config=None,
                 prompt_config_name="default"):
        """
        åˆæœŸåŒ–
        
        Args:
            lm_studio_url: LM Studioã®URL
            face_server_url: ã‚·ãƒªã‚¦ã‚¹è¡¨æƒ…ã‚µãƒ¼ãƒãƒ¼ã®URL
            voicevox_config: VOICEVOXè¨­å®šè¾æ›¸
            prompt_config_name: ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šå
        """
        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        super().__init__(lm_studio_url, face_server_url, voicevox_config)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
        self.prompt_tuner = PromptTuner(lm_studio_url)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿
        self.current_prompt_config = prompt_config_name
        self.load_prompt_configuration(prompt_config_name)
        
        logger.info(f"âœ… AdvancedLLMFaceControlleråˆæœŸåŒ–å®Œäº† (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š: {prompt_config_name})")
    
    def load_prompt_configuration(self, config_name: str):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            configs = self.prompt_tuner.configs
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
            if config_name in configs["system_messages"]:
                self.system_message = configs["system_messages"][config_name]
                logger.info(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š: {config_name}")
            else:
                logger.warning(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š '{config_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                self.system_message = configs["system_messages"]["default"]
            
            # LLMè¨­å®šã‚’é©ç”¨
            if config_name in configs["llm_settings"]:
                self.llm_settings = configs["llm_settings"][config_name]
                logger.info(f"LLMè¨­å®šã‚’é©ç”¨: {config_name}")
            else:
                logger.warning(f"LLMè¨­å®š '{config_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                self.llm_settings = configs["llm_settings"]["default"]
            
            self.current_prompt_config = config_name
            
        except Exception as e:
            logger.error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.system_message = "ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥çš„ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€Œã‚·ãƒªã‚¦ã‚¹ã€ã§ã™ã€‚è‡ªç„¶ã§è¦ªã—ã¿ã‚„ã™ã„æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
            self.llm_settings = {
                "model": "openai/gpt-oss-20b",
                "temperature": 0.7,
                "max_tokens": -1
            }
    
    def switch_prompt_configuration(self, config_name: str) -> bool:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’åˆ‡ã‚Šæ›¿ãˆ"""
        try:
            old_config = self.current_prompt_config
            self.load_prompt_configuration(config_name)
            logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’åˆ‡ã‚Šæ›¿ãˆ: {old_config} â†’ {config_name}")
            return True
        except Exception as e:
            logger.error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šåˆ‡ã‚Šæ›¿ãˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_llm_response(self, user_message: str) -> Optional[str]:
        """
        LLMã‹ã‚‰å¿œç­”ã‚’å–å¾—ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šé©ç”¨ç‰ˆï¼‰
        
        Args:
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            
        Returns:
            LLMã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            # ä¼šè©±å±¥æ­´ã‚’æ§‹ç¯‰
            messages = [{"role": "system", "content": self.system_message}]
            
            # éå»ã®ä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
            for history_item in self.conversation_history[-self.max_history_length:]:
                messages.append({"role": "user", "content": history_item["user"]})
                messages.append({"role": "assistant", "content": history_item["assistant"]})
            
            # ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
            messages.append({"role": "user", "content": user_message})
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã‚’é©ç”¨ã—ã¦LLMã«é€ä¿¡
            response = self.llm_client.chat_completion(
                messages=messages,
                model=self.llm_settings["model"],
                temperature=self.llm_settings["temperature"],
                max_tokens=self.llm_settings["max_tokens"]
            )
            
            if response and "choices" in response:
                ai_response = response["choices"][0]["message"]["content"]
                
                # ä¼šè©±å±¥æ­´ã«è¿½åŠ 
                self.conversation_history.append({
                    "user": user_message,
                    "assistant": ai_response
                })
                
                # å±¥æ­´é•·åˆ¶é™
                if len(self.conversation_history) > self.max_history_length:
                    self.conversation_history = self.conversation_history[-self.max_history_length:]
                
                logger.info(f"LLMå¿œç­”å–å¾—æˆåŠŸ (è¨­å®š: {self.current_prompt_config}): {ai_response[:50]}...")
                return ai_response
            else:
                logger.error("LLMã‹ã‚‰æœ‰åŠ¹ãªå¿œç­”ãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return None
                
        except Exception as e:
            logger.error(f"LLMå¿œç­”å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def get_available_prompt_configurations(self) -> Dict[str, List[str]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šä¸€è¦§ã‚’å–å¾—"""
        try:
            configs = self.prompt_tuner.configs
            return {
                "system_messages": list(configs["system_messages"].keys()),
                "llm_settings": list(configs["llm_settings"].keys())
            }
        except Exception as e:
            logger.error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"system_messages": [], "llm_settings": []}
    
    def test_current_configuration(self, test_message: str) -> Dict[str, Any]:
        """ç¾åœ¨ã®è¨­å®šã§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        try:
            result = self.prompt_tuner.test_prompt_combination(
                system_message_name=self.current_prompt_config,
                llm_setting_name=self.current_prompt_config,
                user_message=test_message,
                scenario_name="current_config_test"
            )
            
            logger.info(f"è¨­å®šãƒ†ã‚¹ãƒˆå®Œäº†: {self.current_prompt_config}")
            return result
            
        except Exception as e:
            logger.error(f"è¨­å®šãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "user_message": test_message
            }
    
    def optimize_for_scenario(self, scenario_name: str) -> bool:
        """ç‰¹å®šã®ã‚·ãƒŠãƒªã‚ªã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šã«åˆ‡ã‚Šæ›¿ãˆ"""
        try:
            # ãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æ
            analysis = self.prompt_tuner.analyze_results(scenario_name)
            
            if "error" in analysis or not analysis["best_performing_settings"]:
                logger.warning(f"ã‚·ãƒŠãƒªã‚ª '{scenario_name}' ã®æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                return False
            
            # æœ€é«˜æ€§èƒ½ã®è¨­å®šã‚’å–å¾—
            best_setting = analysis["best_performing_settings"][0][0]
            
            # è¨­å®šåã‚’è§£æ (format: "system_message_name Ã— llm_setting_name")
            if " Ã— " in best_setting:
                sys_msg_name, llm_setting_name = best_setting.split(" Ã— ", 1)
                
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨LLMè¨­å®šã‚’å€‹åˆ¥ã«é©ç”¨
                configs = self.prompt_tuner.configs
                if sys_msg_name in configs["system_messages"]:
                    self.system_message = configs["system_messages"][sys_msg_name]
                
                if llm_setting_name in configs["llm_settings"]:
                    self.llm_settings = configs["llm_settings"][llm_setting_name]
                
                logger.info(f"ã‚·ãƒŠãƒªã‚ª '{scenario_name}' ã«æœ€é©åŒ–: {best_setting}")
                return True
            else:
                logger.error(f"è¨­å®šåã®è§£æã«å¤±æ•—: {best_setting}")
                return False
                
        except Exception as e:
            logger.error(f"ã‚·ãƒŠãƒªã‚ªæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®è¨­å®šã®æ€§èƒ½çµ±è¨ˆã‚’å–å¾—"""
        try:
            # ç¾åœ¨ã®è¨­å®šã«é–¢é€£ã™ã‚‹ãƒ†ã‚¹ãƒˆçµæœã‚’æŠ½å‡º
            current_setting_key = f"{self.current_prompt_config} Ã— {self.current_prompt_config}"
            
            related_results = [
                result for result in self.prompt_tuner.test_results
                if f"{result['system_message_name']} Ã— {result['llm_setting_name']}" == current_setting_key
            ]
            
            if not related_results:
                return {"error": "ç¾åœ¨ã®è¨­å®šã«é–¢ã™ã‚‹ãƒ†ã‚¹ãƒˆçµæœãŒã‚ã‚Šã¾ã›ã‚“"}
            
            successful_results = [r for r in related_results if r["success"]]
            
            stats = {
                "configuration": current_setting_key,
                "total_tests": len(related_results),
                "successful_tests": len(successful_results),
                "success_rate_percent": len(successful_results) / len(related_results) * 100,
                "average_response_time": sum(r["response_time_seconds"] for r in successful_results) / len(successful_results) if successful_results else 0,
                "recent_tests": related_results[-5:]  # æœ€æ–°5ä»¶
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"æ€§èƒ½çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    async def process_user_input_with_optimization(self, 
                                                   user_message: str, 
                                                   expression: str = "happy",
                                                   auto_optimize: bool = False,
                                                   scenario_hint: Optional[str] = None) -> Dict[str, Any]:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å‡¦ç†ï¼ˆè‡ªå‹•æœ€é©åŒ–æ©Ÿèƒ½ä»˜ãï¼‰
        
        Args:
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            expression: è¨­å®šã™ã‚‹è¡¨æƒ…
            auto_optimize: è‡ªå‹•æœ€é©åŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
            scenario_hint: ã‚·ãƒŠãƒªã‚ªã®ãƒ’ãƒ³ãƒˆï¼ˆæœ€é©åŒ–ç”¨ï¼‰
            
        Returns:
            å‡¦ç†çµæœè¾æ›¸
        """
        # è‡ªå‹•æœ€é©åŒ–ãŒæœ‰åŠ¹ãªå ´åˆ
        if auto_optimize and scenario_hint:
            logger.info(f"è‡ªå‹•æœ€é©åŒ–ã‚’å®Ÿè¡Œ: {scenario_hint}")
            self.optimize_for_scenario(scenario_hint)
        
        # é€šå¸¸ã®å‡¦ç†ã‚’å®Ÿè¡Œ
        return await self.process_user_input(user_message, expression)

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆé–¢æ•°
async def test_advanced_controller():
    """AdvancedLLMFaceControllerã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª AdvancedLLMFaceControllerãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼åˆæœŸåŒ–
    controller = AdvancedLLMFaceController(prompt_config_name="default")
    
    if not controller.is_initialized:
        print("âŒ ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—")
        return
    
    # åˆ©ç”¨å¯èƒ½ãªè¨­å®šã‚’ç¢ºèª
    available_configs = controller.get_available_prompt_configurations()
    print(f"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªè¨­å®š: {available_configs}")
    
    # ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    test_messages = [
        "ã“ã‚“ã«ã¡ã¯ï¼",
        "Pythonã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’æ•™ãˆã¦ãã ã•ã„",
        "çŸ­ã„ç‰©èªã‚’ä½œã£ã¦ãã ã•ã„"
    ]
    
    # å„è¨­å®šã§ãƒ†ã‚¹ãƒˆ
    for sys_msg_name in available_configs["system_messages"][:3]:  # æœ€åˆã®3ã¤ã®è¨­å®šã‚’ãƒ†ã‚¹ãƒˆ
        print(f"\nğŸ”§ è¨­å®šåˆ‡ã‚Šæ›¿ãˆ: {sys_msg_name}")
        
        success = controller.switch_prompt_configuration(sys_msg_name)
        if not success:
            print(f"âŒ è¨­å®šåˆ‡ã‚Šæ›¿ãˆå¤±æ•—: {sys_msg_name}")
            continue
        
        # ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§å®Ÿè¡Œ
        for message in test_messages[:1]:  # ç°¡ç•¥åŒ–ã®ãŸã‚1ã¤ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿
            print(f"ğŸ“ ãƒ†ã‚¹ãƒˆ: {message}")
            
            result = await controller.process_user_input_with_optimization(
                user_message=message,
                expression="happy",
                auto_optimize=False
            )
            
            if result["success"]:
                print(f"âœ… æˆåŠŸ: {result['llm_response'][:100]}...")
            else:
                print(f"âŒ å¤±æ•—: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
    
    # æ€§èƒ½çµ±è¨ˆã‚’è¡¨ç¤º
    stats = controller.get_performance_stats()
    print(f"\nğŸ“Š ç¾åœ¨ã®è¨­å®šã®æ€§èƒ½çµ±è¨ˆ: {stats}")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    controller.cleanup()
    print("\nğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ Advanced LLMFaceController")
    print("=" * 50)
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(test_advanced_controller())

if __name__ == "__main__":
    main()